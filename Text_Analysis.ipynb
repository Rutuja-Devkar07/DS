{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bdf58f-ff4d-4640-aff8-31d845d04836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      " Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
      "\n",
      "Tokens:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.']\n",
      "\n",
      "POS Tags:\n",
      " [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('.', '.')]\n",
      "\n",
      "After Stopwords Removal:\n",
      " ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'Artificial', 'Intelligence']\n",
      "\n",
      "After Stemming:\n",
      " ['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'artifici', 'intellig']\n",
      "\n",
      "After Lemmatization:\n",
      " ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'Artificial', 'Intelligence']\n",
      "\n",
      "Manual TF-IDF DataFrame:\n",
      "        Mars   is   largest    fourth      from  the    Planet       Sun  \\\n",
      "0  0.000000  0.0  0.138629  0.000000  0.000000  0.0  0.138629  0.000000   \n",
      "1  0.086643  0.0  0.000000  0.086643  0.086643  0.0  0.000000  0.086643   \n",
      "\n",
      "     planet   Jupiter  \n",
      "0  0.000000  0.138629  \n",
      "1  0.086643  0.000000  \n",
      "\n",
      "Feature Names and their corresponding TF-IDF values for Document 1:\n",
      "fourth: 0.0\n",
      "from: 0.0\n",
      "is: 0.3793034928087496\n",
      "jupiter: 0.5330978245262535\n",
      "largest: 0.5330978245262535\n",
      "mars: 0.0\n",
      "planet: 0.3793034928087496\n",
      "sun: 0.0\n",
      "the: 0.3793034928087496\n",
      "\n",
      "TF-IDF Matrix using TfidfVectorizer:\n",
      "      fourth      from        is   jupiter   largest      mars    planet  \\\n",
      "0  0.000000  0.000000  0.379303  0.533098  0.533098  0.000000  0.379303   \n",
      "1  0.376957  0.376957  0.268208  0.000000  0.000000  0.376957  0.268208   \n",
      "\n",
      "        sun       the  \n",
      "0  0.000000  0.379303  \n",
      "1  0.376957  0.536416  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\devka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# DSBDA_PR7: Text Analytics\n",
    "\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1) Document Preprocessing\n",
    "# Sample Document\n",
    "document = \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\"\n",
    "\n",
    "print(\"Original Document:\\n\", document)\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags)\n",
    "\n",
    "# Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"\\nAfter Stopwords Removal:\\n\", filtered_tokens)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Stemming:\\n\", stemmed_words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Lemmatization:\\n\", lemmatized_words)\n",
    "\n",
    "# 2) TF, IDF, TF-IDF Calculation (Manual)\n",
    "\n",
    "# Sample Corpus\n",
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'\n",
    "\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "# Calculate TF, IDF, TF-IDF\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "# Display in DataFrame\n",
    "df_manual = pd.DataFrame([tfidfA, tfidfB])\n",
    "print(\"\\nManual TF-IDF DataFrame:\\n\", df_manual)\n",
    "\n",
    "# 3) TF-IDF using TfidfVectorizer (Scikit-learn)\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [documentA, documentB]\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert matrix to array\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Display feature names and values for Document 1\n",
    "print(\"\\nFeature Names and their corresponding TF-IDF values for Document 1:\")\n",
    "for idx, word in enumerate(feature_names):\n",
    "    print(f\"{word}: {tfidf_array[0][idx]}\")\n",
    "\n",
    "# Optionally, display the full matrix as a DataFrame\n",
    "df_vectorizer = pd.DataFrame(tfidf_array, columns=feature_names)\n",
    "print(\"\\nTF-IDF Matrix using TfidfVectorizer:\\n\", df_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289c6a0-7777-4252-881d-83777727cb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
