# -*- coding: utf-8 -*-
"""DSBDA_PR7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1FXNCti5W_LhRXj9htMR-VBXxrKgLqI
"""

# 7) Text Analytics
# 1. Extract Sample document and apply following document preprocessing methods:
# Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.
# 2. Create representation of document by calculating Term Frequency and Inverse Document
# Frequency


# Import necessary libraries
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required NLTK data
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

!pip install nltk

# Sample Document
document = "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence."

print("Original Document:\n", document)

# 1. Tokenization
tokens = word_tokenize(document)
print("\nTokens:\n", tokens)

# 2. POS Tagging
pos_tags = pos_tag(tokens)
print("\nPOS Tags:\n", pos_tags)

# 3. Stopwords Removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]
print("\nAfter Stopwords Removal:\n", filtered_tokens)

# 4. Stemming
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
print("\nAfter Stemming:\n", stemmed_words)

# 5. Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("\nAfter Lemmatization:\n", lemmatized_words)

import pandas as pd
import math
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample Document (you can add more documents to create a corpus)
documentA = 'Jupiter is the largest Planet'
documentB = 'Mars is the fourth planet from the Sun'

bagOfWordsA = documentA.split(' ')
bagOfWordsB = documentB.split(' ')

uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))

numOfWordsA = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsA:
   numOfWordsA[word] += 1
   numOfWordsB = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsB:
    numOfWordsB[word] += 1

def computeTF(wordDict, bagOfWords):
  tfDict = {}
  bagOfWordsCount = len(bagOfWords)
  for word, count in wordDict.items():
   tfDict[word] = count / float(bagOfWordsCount)
  return tfDict
tfA = computeTF(numOfWordsA, bagOfWordsA)
tfB = computeTF(numOfWordsB, bagOfWordsB)

def computeIDF(documents):
 N = len(documents)
 idfDict = dict.fromkeys(documents[0].keys(), 0)
 for document in documents:
  for word, val in document.items():
   if val > 0:
     idfDict[word] += 1
 for word, val in idfDict.items():
     idfDict[word] = math.log(N / float(val))
 return idfDict
idfs = computeIDF([numOfWordsA, numOfWordsB])
idfs

def computeTFIDF(tfBagOfWords, idfs):
 tfidf = {}
 for word, val in tfBagOfWords.items():
   tfidf[word] = val * idfs[word]
 return tfidf
tfidfA = computeTFIDF(tfA, idfs)
tfidfB = computeTFIDF(tfB, idfs)
df = pd.DataFrame([tfidfA, tfidfB])
df

# Display the feature names (terms) and their corresponding TF-IDF values for the first document
print("\nFeature Names and their corresponding TF-IDF values for Document 1:")
for idx, word in enumerate(feature_names):
    print(f"{word}: {tfidf_matrix[0][idx]}")  # for first document

